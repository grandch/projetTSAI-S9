{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess1(img):\n",
    "    img = np.clip(img, -20, 5)\n",
    "    img = 10 ** (img / 20.0)\n",
    "    return np.clip(img, 0, 255)\n",
    "\n",
    "def preProcess2(img):\n",
    "    log_transformed_values = np.log(img)\n",
    "    percentiles = np.percentile(log_transformed_values, q=[0, 25, 50, 75, 100])\n",
    "    remapped_values = 1 / (1 + np.exp(-(log_transformed_values - percentiles[1])))\n",
    "    return torch.from_numpy(remapped_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatialAug1(sentinel1):\n",
    "    vv = torch.tensor(sentinel1[0])\n",
    "    vh = torch.tensor(sentinel1[1])\n",
    "\n",
    "    i1 = np.random.randint(low=0, high=vv.shape[0]-2, size=None)\n",
    "    i2 = np.random.randint(low=i1+1, high=vv.shape[0]-1, size=None)\n",
    "    j1 = np.random.randint(low=0, high=vv.shape[1]-2, size=None)\n",
    "    j2 = np.random.randint(low=j1+1, high=vv.shape[1]-1, size=None)\n",
    "\n",
    "    vv_croped = vv[i1:i2, j1:j2]\n",
    "    vh_croped = vh[i1:i2, j1:j2]\n",
    "\n",
    "    vv_resized = cv2.resize(vv_croped.numpy(), dsize=(512, 512))\n",
    "    vh_resized = cv2.resize(vh_croped.numpy(), dsize=(512, 512))\n",
    "    fill = np.zeros((512,512))\n",
    "\n",
    "    return torch.from_numpy(np.stack((vv_resized, vh_resized, fill), axis=0))\n",
    "\n",
    "def spatialAug2(sentinel2):\n",
    "    r = torch.tensor(sentinel2[0])\n",
    "    g = torch.tensor(sentinel2[1])\n",
    "    b = torch.tensor(sentinel2[2])\n",
    "\n",
    "    i1 = np.random.randint(low=0, high=r.shape[0]-2, size=None)\n",
    "    i2 = np.random.randint(low=i1+1, high=r.shape[0]-1, size=None)\n",
    "    j1 = np.random.randint(low=0, high=r.shape[1]-2, size=None)\n",
    "    j2 = np.random.randint(low=j1+1, high=r.shape[1]-1, size=None)\n",
    "\n",
    "    r_croped = r[i1:i2, j1:j2]\n",
    "    g_croped = g[i1:i2, j1:j2]\n",
    "    b_croped = b[i1:i2, j1:j2]\n",
    "\n",
    "    r_resized = cv2.resize(r_croped.numpy(), dsize=(512, 512))\n",
    "    g_resized = cv2.resize(g_croped.numpy(), dsize=(512, 512))\n",
    "    b_resized = cv2.resize(b_croped.numpy(), dsize=(512, 512))\n",
    "\n",
    "    return torch.from_numpy(np.stack((r_resized, g_resized, b_resized), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/charlie/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/charlie/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "resnet1  = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "resnet2  = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "\n",
    "class ProjectionHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, output_dim)\n",
    "        self.norm = torch.nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.norm(x)\n",
    "        x = torch.nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "def criterion(x, y):\n",
    "    l = torch.empty(len(x))\n",
    "    for i in range(len(l)):\n",
    "        lxyi = torch.exp(torch.nn.functional.cosine_similarity(x[i], y[i], dim=0))\n",
    "        lyxi = torch.exp(torch.nn.functional.cosine_similarity(y[i], x[i], dim=0))\n",
    "\n",
    "        simxy, simxx, simyx, simyy = 0,0,0,0\n",
    "        for j in range(len(x)):\n",
    "            simxy += torch.exp(torch.nn.functional.cosine_similarity(x[i], y[j], dim=0))\n",
    "            simyx += torch.exp(torch.nn.functional.cosine_similarity(y[i], x[j], dim=0))\n",
    "            if j!=i:\n",
    "                simxx += torch.exp(torch.nn.functional.cosine_similarity(x[i], x[j], dim=0))\n",
    "                simyy += torch.exp(torch.nn.functional.cosine_similarity(y[i], y[j], dim=0))\n",
    "               \n",
    "\n",
    "        l[i] = -torch.log(lxyi/(simxx + simxy)) + -torch.log(lyxi/(simyy + simyx))\n",
    "    return torch.sum(l)\n",
    "\n",
    "def preTrain(model, batch, weight_decay=0.0, optimizer=\"sgd\", learning_rate=0.1, momentum=0.9, num_epochs=10):\n",
    "    g1 = ProjectionHead(1000)\n",
    "    g2 = ProjectionHead(1000)\n",
    "    assert optimizer in (\"sgd\", \"adam\")\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer1 = torch.optim.SGD(model[0].parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        optimizer2 = torch.optim.SGD(model[1].parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        optimizerG1 = torch.optim.SGD(g1.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        optimizerG2 = torch.optim.SGD(g2.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer1 = torch.optim.Adam(model[0].parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        optimizer2 = torch.optim.Adam(model[1].parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        optimizerG1 = torch.optim.Adam(g1.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        optimizerG2 = torch.optim.Adam(g2.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    iters, losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        batch = np.random.shuffle(batch)\n",
    "        for imgs in batch:\n",
    "            imgs1 = imgs[0].to(device)\n",
    "            imgs2 = imgs[1].to(device)\n",
    "\n",
    "            model[0].float()\n",
    "            model[1].float()\n",
    "\n",
    "            model[0].train()\n",
    "            model[1].train()\n",
    "            g1.train()\n",
    "            g2.train()\n",
    "\n",
    "            x = model[0](imgs1.unsqueeze(0).float())\n",
    "            y = model[1](imgs2.unsqueeze(0).float())\n",
    "            x = g1(x)\n",
    "            y = g2(y)\n",
    "\n",
    "            loss = criterion(x, y)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer1.step()\n",
    "            optimizer1.zero_grad()\n",
    "            optimizer2.step()\n",
    "            optimizer2.zero_grad()\n",
    "            optimizerG1.step()\n",
    "            optimizerG1.zero_grad()\n",
    "            optimizerG2.step()\n",
    "            optimizerG2.zero_grad()\n",
    "\n",
    "            losses.append(float(loss)/len(batch))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import glob\n",
    "\n",
    "def makePair(path1, path2):\n",
    "    # add test\n",
    "    sentinel1_image_path = path1\n",
    "    sentinel2_image_path = path2\n",
    "\n",
    "    sentinel1 = rasterio.open(sentinel1_image_path)\n",
    "    sentinel2 = rasterio.open(sentinel2_image_path)\n",
    "\n",
    "    vv = np.array(sentinel1.read(1))\n",
    "    vh = np.array(sentinel1.read(2))\n",
    "    r = np.array(sentinel2.read(3))\n",
    "    g = np.array(sentinel2.read(2))\n",
    "    b = np.array(sentinel2.read(1))\n",
    "\n",
    "    img1 = spatialAug1(preProcess1(np.stack((vv, vh))))\n",
    "    img2 = spatialAug2(preProcess2(np.stack((r,g,b))))\n",
    "    return [img1, img2]\n",
    "\n",
    "def makeBatch(path):\n",
    "    filenames = next(walk(path), (None, None, []))[2]\n",
    "    filenames = np.sort(filenames)\n",
    "    batch = []\n",
    "    for i in range(0,len(filenames),2):\n",
    "        batch.append(makePair(path+filenames[i], path+filenames[i+1]))\n",
    "    return batch\n",
    "\n",
    "batch = makeBatch(\"./samples/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = preTrain((resnet1, resnet2), batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(10*len(batch)), losses, label='Loss evolution')\n",
    "plt.xlabel(\"Iteration count\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
